{"text_description": "transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration.forward contains transformers.modeling_flash_attention_utils._flash_attention_forward <class 'traincheck.invariant.base_cls._NOT_SET'>", "relation": "APIContainRelation", "params": [{"param_type": "APIParam", "api_full_name": "transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration.forward", "arguments": null}, {"param_type": "APIParam", "api_full_name": "transformers.modeling_flash_attention_utils._flash_attention_forward", "arguments": {"args": {"dropout": {"float": 0.0}, "is_causal": {"bool": true}, "use_top_left_mask": {"bool": false}, "query_states": {"torch.cuda.HalfTensor": {"dtype": "torch.float16", "is_cpu": false, "is_cuda": true, "is_ipu": false, "is_leaf": true, "is_meta": false, "is_mkldnn": false, "is_mps": false, "is_mtia": false, "is_nested": false, "is_ort": false, "is_quantized": false, "is_sparse": false, "is_sparse_csr": false, "is_vulkan": false, "is_xla": false, "is_xpu": false, "itemsize": 2, "nbytes": 20480, "ndim": 4, "requires_grad": false, "retains_grad": false}}, "key_states": {"torch.cuda.HalfTensor": {"dtype": "torch.float16", "is_cpu": false, "is_cuda": true, "is_ipu": false, "is_leaf": true, "is_meta": false, "is_mkldnn": false, "is_mps": false, "is_mtia": false, "is_nested": false, "is_ort": false, "is_quantized": false, "is_sparse": false, "is_sparse_csr": false, "is_vulkan": false, "is_xla": false, "is_xpu": false, "itemsize": 2, "nbytes": 20480, "ndim": 4, "requires_grad": false, "retains_grad": false}}, "value_states": {"torch.cuda.HalfTensor": {"dtype": "torch.float16", "is_cpu": false, "is_cuda": true, "is_ipu": false, "is_leaf": true, "is_meta": false, "is_mkldnn": false, "is_mps": false, "is_mtia": false, "is_nested": false, "is_ort": false, "is_quantized": false, "is_sparse": false, "is_sparse_csr": false, "is_vulkan": false, "is_xla": false, "is_xpu": false, "itemsize": 2, "nbytes": 20480, "ndim": 4, "requires_grad": false, "retains_grad": false}}, "attention_mask": {"NoneType": null}, "query_length": {"int": 10}}, "func_name": "transformers.modeling_flash_attention_utils._flash_attention_forward"}}], "precondition": {"parent_func_call_pre": {"inverted": false, "preconditions": [{"clauses": [{"type": "constant", "prop_name": "meta_vars.stage", "additional_path": "None", "prop_dtype": "str", "values": ["testing"]}]}]}}, "num_positive_examples": 72, "num_negative_examples": 0}